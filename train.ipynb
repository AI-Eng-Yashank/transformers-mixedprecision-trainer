{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model Fine-Tuning\n",
    "\n",
    "This notebook fine-tunes a Seq2Seq model using Hugging Face Transformers.\n",
    "It loads a dataset, tokenizes the data, sets up training arguments, and trains the model.\n",
    "Mixed precision and checkpoint management are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Trainer, TrainingArguments, AdamW, get_scheduler\n",
    ")\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"chibbss/fitness-chat-prompt-completion-dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"D:/cuda/final_model\"  # Change this to your model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = examples['instruction']\n",
    "    targets = examples['output']\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset['train'].map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['output', 'instruction'])\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Model is running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Custom Trainer with Checkpoint Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def save_checkpoint(self, output_dir=None):\n",
    "        super().save_checkpoint(output_dir)\n",
    "        checkpoints = sorted(\n",
    "            [ckpt for ckpt in os.listdir(self.args.output_dir) if ckpt.startswith(\"checkpoint\")],\n",
    "            key=lambda x: int(x.split(\"-\")[-1])\n",
    "        )\n",
    "        if len(checkpoints) > 5:\n",
    "            for ckpt_to_delete in checkpoints[:-5]:\n",
    "                shutil.rmtree(os.path.join(self.args.output_dir, ckpt_to_delete))\n",
    "                print(f\"Deleted old checkpoint: {ckpt_to_delete}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Mixed Precision Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecisionTrainer(CustomTrainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        with autocast(\"cuda\"):\n",
    "            return super().training_step(model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "num_training_steps = len(tokenized_datasets['train']) // (\n",
    "    training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    ") * training_args.num_train_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MixedPrecisionTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Resume from Checkpoint (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir):\n",
    "    last_checkpoint = max(\n",
    "        [\n",
    "            os.path.join(training_args.output_dir, ckpt)\n",
    "            for ckpt in os.listdir(training_args.output_dir)\n",
    "            if ckpt.startswith(\"checkpoint\")\n",
    "        ],\n",
    "        key=os.path.getctime,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "if last_checkpoint:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=last_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
